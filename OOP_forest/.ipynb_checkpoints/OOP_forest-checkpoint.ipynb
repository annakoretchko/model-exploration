{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "\n",
    "The data for this analysis is of the Pima Indians and their liklihood of diabetetes. The data can be found here: https://data.world/uci/pima-indians-diabetes\n",
    "\n",
    "This script implements a random forest classifier where the classifier will be evaluated via the out-of-bag (OOB) error estimate, using the above dataset.\n",
    "\n",
    "Each tree in the forest is constructed using a different bootstrap sample from the original data.\n",
    "\n",
    "\n",
    "**ENJOY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "import ast\n",
    "from datetime import datetime\n",
    "from math import log, floor, ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility(object):\n",
    "    \"\"\"\n",
    "    Class with the necessary utility functions\n",
    "    \"\"\"\n",
    "    def entropy(self, class_y):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method compute the entropy for a list of classes\n",
    "        \n",
    "        Input:            \n",
    "           class_y         : list of class labels (0's and 1's)\n",
    "        \n",
    "        Output:\n",
    "            entropy\n",
    "        \"\"\"\n",
    "\n",
    "        entropy = 0\n",
    "        val = np.mean(class_y)\n",
    "        \n",
    "        if val == 0:\n",
    "            entropy =  entropy\n",
    "        elif val == 1:\n",
    "            entropy = entropy\n",
    "        else:\n",
    "            entropy = (-val)*np.log2(val) - (1-val) * np.log2(1-val)\n",
    "        \n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def partition_classes(self, X, y, split_attribute, split_val):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method partitions the classes. \n",
    "        First check if the split attribute is numerical or categorical    \n",
    "        If the split attribute is numeric, split_val should be a numerical value\n",
    "        For example, your split_val could be the mean of the values of split_attribute\n",
    "        If the split attribute is categorical, split_val should be one of the categories.   \n",
    "        \n",
    "        Inputs:\n",
    "           X               : data containing all attributes\n",
    "           y               : labels\n",
    "           split_attribute : column index of the attribute to split on\n",
    "           split_val       : either a numerical or categorical value to divide the split_attribute\n",
    "           \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        X_left = []\n",
    "        X_right = []\n",
    "\n",
    "        y_left = []\n",
    "        y_right = []\n",
    "\n",
    "\n",
    "        l = X[:,split_attribute] <= split_val\n",
    "        r = X[:,split_attribute] > split_val\n",
    "        \n",
    "        X_left = X[l,:]\n",
    "        X_right = X[r,:]\n",
    "\n",
    "        y_left = y[l]\n",
    "        y_right = y[r]\n",
    "\n",
    "        return (X_left, X_right, y_left, y_right)\n",
    "    \n",
    "\n",
    "\n",
    "    def information_gain(self, previous_y, current_y):\n",
    "        \n",
    "        \"\"\"\n",
    "        This metho computes and returns the information gain from partitioning the previous_y labels\n",
    "        into the current_y labels.\n",
    "        \n",
    "        Inputs:\n",
    "           previous_y: the distribution of original labels (0's and 1's)\n",
    "           current_y:  the distribution of labels after splitting based on a particular\n",
    "                       split attribute and split value\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        info_gain = 0\n",
    "\n",
    "        val = len(current_y[0])/len(previous_y)\n",
    "        \n",
    "        if val == 0:\n",
    "            info_gain =  info_gain\n",
    "        elif val == 1:\n",
    "            info_gain = info_gain\n",
    "        else:\n",
    "            info_gain = Utility.entropy(self,previous_y) - Utility.entropy(self,current_y[0])*val + (1 - val) * Utility.entropy(self,current_y[1])\n",
    "\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        For each node find the best split criteria and return the \n",
    "        split attribute, spliting value along with \n",
    "        X_left, X_right, y_left, y_right (using partition_classes)\n",
    "        \n",
    "         Inputs:\n",
    "           X       : Data containing all attributes\n",
    "           y       : labels\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        split_attribute = 0\n",
    "        split_value = 0\n",
    "        X_left, X_right, y_left, y_right = [], [], [], []\n",
    " \n",
    "        baseline = -1\n",
    "        for b in range(len(X)):\n",
    "            split_value = X[b][split_attribute]\n",
    "            X_left, X_right, y_left, y_right = Utility.partition_classes(self,X,y,split_attribute,split_value)\n",
    "            info_gain = Utility.information_gain(self,y,[y_left,y_right])\n",
    "            \n",
    "            if not np.isnan(info_gain) and info_gain > baseline:\n",
    "                baseline = info_gain\n",
    "                best_split_value = split_value\n",
    "                \n",
    "            \n",
    "        return best_split_value, baseline\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    def best_feature(self,X,y):\n",
    "        \"\"\"\n",
    "        Gets best feature for the model. \n",
    "        \n",
    "        Inputs:\n",
    "           X       : Data containing all attributes\n",
    "           y       : labels\n",
    "           \n",
    "        \"\"\"\n",
    "        \n",
    "        only_now = -1\n",
    "        for b in range(len(X[0])):\n",
    "            only_now_val, info_gain = Utility.best_split(self,X,y)\n",
    "            if not np.isnan(info_gain) and info_gain >only_now:\n",
    "                only_now = info_gain\n",
    "                best_s_feature = b\n",
    "                best_s_val = only_now_val\n",
    "        return best_s_feature,best_s_val\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth):\n",
    "        \"\"\"\n",
    "        Initializing the tree as an empty list\n",
    "        \n",
    "        \"\"\"\n",
    "        self.tree = np.empty((0,4), int) #need list of list ie array to match input of append I use later\n",
    "        self.max_depth = max_depth\n",
    "        self.leaf_size = 10\n",
    "        \n",
    "\n",
    "    def learn(self, X, y, par_node = {}, depth=0):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method trains the decision tree (self.tree) using the the sample X and labels y\n",
    "        and the methods from the Utility class\n",
    "        \"\"\"\n",
    " \n",
    "\n",
    "        if len(y) <= self.leaf_size:\n",
    "            self.tree = np.append(self.tree, [[\"leaf\", np.mean(y), np.nan, np.nan]], axis = 0) if hasattr(self, 'tree') else np.array([[\"leaf\", 1, np.nan, np.nan]])\n",
    "            self.addNext()\n",
    "        elif (y == y[0]).all():\n",
    "            self.tree = np.append(self.tree, [[\"leaf\", y[0], np.nan, np.nan]], axis = 0) if hasattr(self, 'tree') else np.array([[\"leaf\", y[0], np.nan, np.nan]])\n",
    "            self.addNext()\n",
    "        else:\n",
    "            temp = -1\n",
    "            for i in range(X.shape[1]):\n",
    "                SplitVal = np.median(X[:, i])\n",
    "                X_left, X_right, y_left, y_right = Utility.partition_classes(self,X, y, i, SplitVal)\n",
    "                IG = Utility.information_gain(self,y, [y_left, y_right])\n",
    "                if not np.isnan(IG) and IG > temp:\n",
    "                    temp = IG\n",
    "                    index = i\n",
    "            SplitVal = np.median(X[:, index])\n",
    "            X_left, X_right, y_left, y_right = Utility.partition_classes(self,X, y, index, SplitVal)\n",
    "\n",
    "            if len(X_left) == 0 or len(X_right) == 0:\n",
    "                self.tree = np.append(self.tree, [[\"leaf\", np.mean(y), np.nan, np.nan]], axis = 0) if hasattr(self, 'tree') else np.array([[\"leaf\", np.mean(y), np.nan, np.nan]])\n",
    "                self.addNext()\n",
    "            else:\n",
    "                self.tree = np.append(self.tree, [[index, SplitVal, 1, None]], axis = 0) if hasattr(self, 'tree') else np.array([[index, SplitVal, 1, None]])\n",
    "                self.learn(X_left, y_left)\n",
    "                self.learn(X_right, y_right)\n",
    "\n",
    "    def addNext(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Method used to add the next value for the tree conditionally\n",
    "        \"\"\"\n",
    "        n = self.tree.shape[0]\n",
    "        for i in range(n):\n",
    "            if self.tree[n-1-i, 3] == None:\n",
    "                self.tree[n-1-i, 3] = i + 1\n",
    "                break\n",
    "\n",
    "    def classify(self, record):\n",
    "        \n",
    "        \"\"\"\n",
    "        Method to classify the record using self.tree and return the predicted label\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        curr = 0\n",
    "        while True:\n",
    "            if self.tree[curr, 0] == \"leaf\": #want to keep adding if the leaf is there and then increase by .5 as the voting rank and p\n",
    "                return int(float(self.tree[curr, 1]) + 0.5)\n",
    "            else:\n",
    "                curr += self.tree[curr, 2] if record[self.tree[curr, 0]] <= self.tree[curr, 1] else self.tree[curr, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Here, \n",
    "    1. X is assumed to be a matrix with n rows and d columns where n is the\n",
    "    number of total records and d is the number of features of each record. \n",
    "    2. y is assumed to be a vector of labels of length n.\n",
    "    3. XX is similar to X, except that XX also contains the data label for each\n",
    "    record.\n",
    "    \"\"\"\n",
    "\n",
    "    num_trees = 0\n",
    "    decision_trees = []\n",
    "\n",
    "    # the bootstrapping datasets for trees\n",
    "    # bootstraps_datasets is a list of lists, where each list in bootstraps_datasets is a bootstrapped dataset.\n",
    "    bootstraps_datasets = []\n",
    "\n",
    "    # the true class labels, corresponding to records in the bootstrapping datasets\n",
    "    # bootstraps_labels is a list of lists, where the 'i'th list contains the labels corresponding to records in\n",
    "    # the 'i'th bootstrapped dataset.\n",
    "    bootstraps_labels = []\n",
    "\n",
    "    def __init__(self, num_trees):\n",
    "        \"\"\"\n",
    "        Initialization done here\n",
    "        \"\"\"\n",
    "        self.num_trees = num_trees\n",
    "        self.decision_trees = [DecisionTree(max_depth=10) for i in range(num_trees)]\n",
    "        self.bootstraps_datasets = []\n",
    "        self.bootstraps_labels = []\n",
    "        \n",
    "    def _bootstrapping(self, XX, n):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method creates a sample dataset of size n by sampling with replacement\n",
    "        from the original dataset XX.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "\n",
    "        samples = [] # sampled dataset\n",
    "        labels = []  # class labels for the sampled records\n",
    "\n",
    "        #get XX to array\n",
    "        XX = np.array(XX)\n",
    "        decision = np.random.randint(0,n,size = n)\n",
    "        \n",
    "        samples = XX[:,:-1][decision]\n",
    "        labels = XX[:,-1][decision]\n",
    "\n",
    "        return (samples, labels)\n",
    "\n",
    "    def bootstrapping(self, XX):\n",
    "        \"\"\"\n",
    "        Initializing the bootstap datasets for each tree\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.num_trees):\n",
    "            data_sample, data_label = self._bootstrapping(XX, len(XX))\n",
    "            self.bootstraps_datasets.append(data_sample)\n",
    "            self.bootstraps_labels.append(data_label)\n",
    "\n",
    "    def fitting(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method trains `num_trees` decision trees using the bootstraps datasets\n",
    "        and labels by calling the learn function from your DecisionTree class.\n",
    "        \"\"\"\n",
    "     \n",
    "        \n",
    "        for b in range(self.num_trees):\n",
    "            self.decision_trees[b].learn(self.bootstraps_datasets[b],self.bootstraps_labels[b])\n",
    "\n",
    "\n",
    "    def voting(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method votes for the best results via the following logic\n",
    "        \n",
    "               1. Find the set of trees that consider the record as an\n",
    "                  out-of-bag sample.\n",
    "               2. Predict the label using each of the above found trees.\n",
    "               3. Use majority vote to find the final label for this recod.\n",
    "        \n",
    "        \"\"\"\n",
    "        y = []\n",
    "\n",
    "        for record in X:\n",
    "            votes = []\n",
    "            \n",
    "            for i in range(len(self.bootstraps_datasets)):\n",
    "                dataset = self.bootstraps_datasets[i]\n",
    "                \n",
    "                if record not in dataset:\n",
    "                    OOB_tree = self.decision_trees[i]\n",
    "                    effective_vote = OOB_tree.classify(record)\n",
    "                    votes.append(effective_vote)\n",
    "\n",
    "            counts = np.bincount(votes)\n",
    "\n",
    "            if len(counts) == 0:\n",
    "    \n",
    "                # Handle the case where the record is not an out-of-bag sample\n",
    "                # for any of the trees.\n",
    "                y = np.append(y, self.decision_trees[0].classify(record))\n",
    "            else:\n",
    "                y = np.append(y, np.argmax(counts))\n",
    "                \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize according to my implementation\n",
    "# Ensure Minimum forest_size should be 10\n",
    "forest_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the training: get timing and results to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data\n",
      "__Name: annakoretchko__\n",
      "creating the bootstrap datasets\n",
      "fitting the forest\n",
      "accuracy: 0.7487\n",
      "OOB estimate: 0.2513\n",
      "Execution time: 0:00:00.642248\n"
     ]
    }
   ],
   "source": [
    "# start time \n",
    "start = datetime.now()\n",
    "X = list()\n",
    "y = list()\n",
    "XX = list()  # Contains data features and data labels\n",
    "numerical_cols = set([i for i in range(0, 9)])  # indices of numeric attributes (columns)\n",
    "\n",
    "# Loading data set\n",
    "print(\"reading the data\")\n",
    "with open(\"../data/pima-indians-diabetes.csv\") as f:\n",
    "    next(f, None)\n",
    "    for line in csv.reader(f, delimiter=\",\"):\n",
    "        xline = []\n",
    "        for i in range(len(line)):\n",
    "            if i in numerical_cols:\n",
    "                xline.append(ast.literal_eval(line[i]))\n",
    "            else:\n",
    "                xline.append(line[i])\n",
    "\n",
    "        X.append(xline[:-1])\n",
    "        y.append(xline[-1])\n",
    "        XX.append(xline[:])\n",
    "\n",
    "# Initializing a random forest.\n",
    "randomForest = RandomForest(forest_size)\n",
    "\n",
    "# Creating the bootstrapping datasets\n",
    "print(\"creating the bootstrap datasets\")\n",
    "randomForest.bootstrapping(XX)\n",
    "\n",
    "# Building trees in the forest\n",
    "print(\"fitting the forest\")\n",
    "randomForest.fitting()\n",
    "\n",
    "# Calculating an unbiased error estimation of the random forest\n",
    "# based on out-of-bag (OOB) error estimate.\n",
    "y_predicted = randomForest.voting(X)\n",
    "\n",
    "# Comparing predicted and true labels\n",
    "results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "# Accuracy\n",
    "accuracy = float(results.count(True)) / float(len(results))\n",
    "\n",
    "print(\"accuracy: %.4f\" % accuracy)\n",
    "print(\"OOB estimate: %.4f\" % (1 - accuracy))\n",
    "\n",
    "# end time\n",
    "print(\"Execution time: \" + str(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
